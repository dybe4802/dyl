{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bernstein_Dylan_Exercise_3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Q_XTTCnyf-Bg","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import pandas\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LassoLarsCV\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5rLLgQRgPQz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"6d6697b9-f322-4410-f63e-7c2f4201d365","executionInfo":{"status":"ok","timestamp":1574911189470,"user_tz":420,"elapsed":23839,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XPjAzwxglQUo","colab_type":"code","colab":{}},"source":["csvfile = 'drive/My Drive/Colab Notebooks/finalmaster-ratios.csv'\n","alldata=pd.read_csv(csvfile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbIgp1-Almvx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":409},"outputId":"44c74b63-bfe7-429b-9005-86aeb53aafda","executionInfo":{"status":"ok","timestamp":1574911195418,"user_tz":420,"elapsed":583,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["alldata.head(10)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Purchases</th>\n","      <th>B01001001</th>\n","      <th>B01001002</th>\n","      <th>B01001003</th>\n","      <th>B01001004</th>\n","      <th>B01001005</th>\n","      <th>B01001006</th>\n","      <th>B01001007</th>\n","      <th>B01001008</th>\n","      <th>B01001009</th>\n","      <th>B01001010</th>\n","      <th>B01001011</th>\n","      <th>B01001012</th>\n","      <th>B01001013</th>\n","      <th>B01001014</th>\n","      <th>B01001015</th>\n","      <th>B01001016</th>\n","      <th>B01001017</th>\n","      <th>B01001018</th>\n","      <th>B01001019</th>\n","      <th>B01001020</th>\n","      <th>B01001021</th>\n","      <th>B01001022</th>\n","      <th>B01001023</th>\n","      <th>B01001024</th>\n","      <th>B01001025</th>\n","      <th>B01001026</th>\n","      <th>B01001027</th>\n","      <th>B01001028</th>\n","      <th>B01001029</th>\n","      <th>B01001030</th>\n","      <th>B01001031</th>\n","      <th>B01001032</th>\n","      <th>B01001033</th>\n","      <th>B01001034</th>\n","      <th>B01001035</th>\n","      <th>B01001036</th>\n","      <th>B01001037</th>\n","      <th>B01001038</th>\n","      <th>B01001039</th>\n","      <th>...</th>\n","      <th>B15002013</th>\n","      <th>B15002014</th>\n","      <th>B15002015</th>\n","      <th>B15002016</th>\n","      <th>B15002017</th>\n","      <th>B15002018</th>\n","      <th>B15002019</th>\n","      <th>B15002020</th>\n","      <th>B15002021</th>\n","      <th>B15002022</th>\n","      <th>B15002023</th>\n","      <th>B15002024</th>\n","      <th>B15002025</th>\n","      <th>B15002026</th>\n","      <th>B15002027</th>\n","      <th>B15002028</th>\n","      <th>B15002029</th>\n","      <th>B15002030</th>\n","      <th>B15002031</th>\n","      <th>B15002032</th>\n","      <th>B15002033</th>\n","      <th>B15002034</th>\n","      <th>B15002035</th>\n","      <th>B19001001</th>\n","      <th>B19001002</th>\n","      <th>B19001003</th>\n","      <th>B19001004</th>\n","      <th>B19001005</th>\n","      <th>B19001006</th>\n","      <th>B19001007</th>\n","      <th>B19001008</th>\n","      <th>B19001009</th>\n","      <th>B19001010</th>\n","      <th>B19001011</th>\n","      <th>B19001012</th>\n","      <th>B19001013</th>\n","      <th>B19001014</th>\n","      <th>B19001015</th>\n","      <th>B19001016</th>\n","      <th>B19001017</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22</td>\n","      <td>206252</td>\n","      <td>469.226965</td>\n","      <td>31.432422</td>\n","      <td>35.219052</td>\n","      <td>33.628765</td>\n","      <td>20.121017</td>\n","      <td>12.610787</td>\n","      <td>6.734480</td>\n","      <td>6.225394</td>\n","      <td>19.432539</td>\n","      <td>28.101546</td>\n","      <td>28.421543</td>\n","      <td>26.390047</td>\n","      <td>31.989993</td>\n","      <td>31.359696</td>\n","      <td>32.116052</td>\n","      <td>32.213021</td>\n","      <td>12.184124</td>\n","      <td>18.361034</td>\n","      <td>9.454454</td>\n","      <td>15.175610</td>\n","      <td>16.281054</td>\n","      <td>11.025348</td>\n","      <td>6.230243</td>\n","      <td>4.518744</td>\n","      <td>530.773035</td>\n","      <td>31.999690</td>\n","      <td>34.322091</td>\n","      <td>32.649380</td>\n","      <td>20.101623</td>\n","      <td>12.513818</td>\n","      <td>8.072649</td>\n","      <td>6.021760</td>\n","      <td>22.923414</td>\n","      <td>31.335454</td>\n","      <td>31.558482</td>\n","      <td>31.063941</td>\n","      <td>36.082074</td>\n","      <td>34.845723</td>\n","      <td>...</td>\n","      <td>64.610300</td>\n","      <td>31.449746</td>\n","      <td>58.735313</td>\n","      <td>20.071053</td>\n","      <td>6.726751</td>\n","      <td>5.882267</td>\n","      <td>543.803963</td>\n","      <td>6.974272</td>\n","      <td>2.504332</td>\n","      <td>5.904107</td>\n","      <td>11.917415</td>\n","      <td>10.767170</td>\n","      <td>18.141844</td>\n","      <td>19.779852</td>\n","      <td>10.956451</td>\n","      <td>181.418442</td>\n","      <td>26.717724</td>\n","      <td>85.271036</td>\n","      <td>54.243532</td>\n","      <td>72.647457</td>\n","      <td>30.816383</td>\n","      <td>2.831933</td>\n","      <td>2.912014</td>\n","      <td>1000</td>\n","      <td>105.667996</td>\n","      <td>82.298375</td>\n","      <td>68.141163</td>\n","      <td>67.336195</td>\n","      <td>63.566902</td>\n","      <td>59.439845</td>\n","      <td>49.409690</td>\n","      <td>53.306757</td>\n","      <td>42.318307</td>\n","      <td>83.167229</td>\n","      <td>89.249208</td>\n","      <td>102.141470</td>\n","      <td>52.872330</td>\n","      <td>36.440765</td>\n","      <td>23.446284</td>\n","      <td>21.197485</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>61399</td>\n","      <td>486.538869</td>\n","      <td>22.899396</td>\n","      <td>21.531295</td>\n","      <td>27.036271</td>\n","      <td>16.808091</td>\n","      <td>28.355511</td>\n","      <td>18.192479</td>\n","      <td>13.534422</td>\n","      <td>21.466148</td>\n","      <td>24.886399</td>\n","      <td>23.534585</td>\n","      <td>21.319565</td>\n","      <td>27.101419</td>\n","      <td>30.961416</td>\n","      <td>37.117868</td>\n","      <td>36.466392</td>\n","      <td>12.557208</td>\n","      <td>20.554081</td>\n","      <td>12.182609</td>\n","      <td>15.651721</td>\n","      <td>20.668089</td>\n","      <td>15.961172</td>\n","      <td>10.423623</td>\n","      <td>7.329110</td>\n","      <td>513.461131</td>\n","      <td>18.974250</td>\n","      <td>23.404290</td>\n","      <td>23.892897</td>\n","      <td>17.036108</td>\n","      <td>35.310021</td>\n","      <td>18.534504</td>\n","      <td>17.101256</td>\n","      <td>22.785387</td>\n","      <td>22.150198</td>\n","      <td>22.622518</td>\n","      <td>21.303279</td>\n","      <td>26.971123</td>\n","      <td>32.329517</td>\n","      <td>...</td>\n","      <td>56.929829</td>\n","      <td>46.381727</td>\n","      <td>65.707446</td>\n","      <td>35.509451</td>\n","      <td>16.782205</td>\n","      <td>9.201536</td>\n","      <td>515.086529</td>\n","      <td>3.017306</td>\n","      <td>1.047329</td>\n","      <td>1.371503</td>\n","      <td>6.358785</td>\n","      <td>4.937410</td>\n","      <td>8.303825</td>\n","      <td>9.700264</td>\n","      <td>7.555733</td>\n","      <td>174.155902</td>\n","      <td>25.834123</td>\n","      <td>60.146626</td>\n","      <td>62.440776</td>\n","      <td>76.604658</td>\n","      <td>55.383771</td>\n","      <td>8.977108</td>\n","      <td>9.251409</td>\n","      <td>1000</td>\n","      <td>71.289558</td>\n","      <td>59.062447</td>\n","      <td>54.704688</td>\n","      <td>60.966323</td>\n","      <td>53.012354</td>\n","      <td>60.881706</td>\n","      <td>59.231680</td>\n","      <td>50.093078</td>\n","      <td>40.700626</td>\n","      <td>92.612963</td>\n","      <td>117.363344</td>\n","      <td>113.344051</td>\n","      <td>75.774243</td>\n","      <td>33.000508</td>\n","      <td>33.169741</td>\n","      <td>24.792689</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>73170</td>\n","      <td>489.859232</td>\n","      <td>28.905289</td>\n","      <td>36.271696</td>\n","      <td>28.235616</td>\n","      <td>21.566216</td>\n","      <td>12.218122</td>\n","      <td>7.243406</td>\n","      <td>7.380074</td>\n","      <td>16.933169</td>\n","      <td>24.914582</td>\n","      <td>26.896269</td>\n","      <td>31.802651</td>\n","      <td>30.531639</td>\n","      <td>36.258029</td>\n","      <td>35.998360</td>\n","      <td>33.429001</td>\n","      <td>13.625803</td>\n","      <td>19.406861</td>\n","      <td>12.245456</td>\n","      <td>14.664480</td>\n","      <td>21.169878</td>\n","      <td>15.293153</td>\n","      <td>8.610086</td>\n","      <td>6.259396</td>\n","      <td>510.140768</td>\n","      <td>26.171928</td>\n","      <td>30.681973</td>\n","      <td>31.925653</td>\n","      <td>19.789531</td>\n","      <td>10.072434</td>\n","      <td>5.056717</td>\n","      <td>6.218396</td>\n","      <td>15.757824</td>\n","      <td>24.449911</td>\n","      <td>26.595599</td>\n","      <td>27.210605</td>\n","      <td>37.556376</td>\n","      <td>37.050704</td>\n","      <td>...</td>\n","      <td>54.602613</td>\n","      <td>40.613027</td>\n","      <td>43.363788</td>\n","      <td>12.280185</td>\n","      <td>5.796247</td>\n","      <td>3.438452</td>\n","      <td>523.980745</td>\n","      <td>5.422930</td>\n","      <td>4.224384</td>\n","      <td>11.828274</td>\n","      <td>18.331860</td>\n","      <td>15.089891</td>\n","      <td>21.731015</td>\n","      <td>18.685529</td>\n","      <td>7.014441</td>\n","      <td>155.241183</td>\n","      <td>45.466156</td>\n","      <td>71.185775</td>\n","      <td>65.802142</td>\n","      <td>56.272718</td>\n","      <td>24.580018</td>\n","      <td>1.689753</td>\n","      <td>1.414677</td>\n","      <td>1000</td>\n","      <td>102.538696</td>\n","      <td>82.960331</td>\n","      <td>74.828305</td>\n","      <td>79.133495</td>\n","      <td>66.081252</td>\n","      <td>78.245122</td>\n","      <td>63.996993</td>\n","      <td>47.322923</td>\n","      <td>42.505211</td>\n","      <td>70.420610</td>\n","      <td>90.033143</td>\n","      <td>98.677692</td>\n","      <td>54.703249</td>\n","      <td>20.125056</td>\n","      <td>11.890525</td>\n","      <td>16.537397</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>94</td>\n","      <td>251724</td>\n","      <td>505.585483</td>\n","      <td>32.054949</td>\n","      <td>31.757004</td>\n","      <td>28.102207</td>\n","      <td>18.651380</td>\n","      <td>12.080692</td>\n","      <td>7.035483</td>\n","      <td>7.686991</td>\n","      <td>25.790151</td>\n","      <td>42.129475</td>\n","      <td>35.824951</td>\n","      <td>32.058922</td>\n","      <td>27.677138</td>\n","      <td>33.842621</td>\n","      <td>38.176733</td>\n","      <td>32.722347</td>\n","      <td>12.493842</td>\n","      <td>16.394940</td>\n","      <td>11.504664</td>\n","      <td>15.914255</td>\n","      <td>16.394940</td>\n","      <td>13.196994</td>\n","      <td>8.648361</td>\n","      <td>5.446441</td>\n","      <td>494.414517</td>\n","      <td>33.123580</td>\n","      <td>28.082344</td>\n","      <td>30.171934</td>\n","      <td>16.863708</td>\n","      <td>9.280005</td>\n","      <td>5.390825</td>\n","      <td>5.609318</td>\n","      <td>19.453846</td>\n","      <td>35.614403</td>\n","      <td>32.082757</td>\n","      <td>28.809331</td>\n","      <td>27.911522</td>\n","      <td>32.690566</td>\n","      <td>...</td>\n","      <td>88.227492</td>\n","      <td>44.076261</td>\n","      <td>87.939148</td>\n","      <td>44.404973</td>\n","      <td>9.671057</td>\n","      <td>7.283569</td>\n","      <td>502.912274</td>\n","      <td>4.509700</td>\n","      <td>0.980370</td>\n","      <td>3.552398</td>\n","      <td>5.986021</td>\n","      <td>7.398907</td>\n","      <td>9.740260</td>\n","      <td>10.605292</td>\n","      <td>7.485410</td>\n","      <td>141.242417</td>\n","      <td>43.078591</td>\n","      <td>84.479020</td>\n","      <td>52.069156</td>\n","      <td>89.836451</td>\n","      <td>33.932320</td>\n","      <td>4.129086</td>\n","      <td>3.886877</td>\n","      <td>1000</td>\n","      <td>61.632139</td>\n","      <td>46.526521</td>\n","      <td>48.437595</td>\n","      <td>54.221644</td>\n","      <td>51.680322</td>\n","      <td>60.066684</td>\n","      <td>54.790900</td>\n","      <td>48.681562</td>\n","      <td>43.873381</td>\n","      <td>84.717507</td>\n","      <td>112.204444</td>\n","      <td>127.137252</td>\n","      <td>83.019904</td>\n","      <td>43.731067</td>\n","      <td>38.851729</td>\n","      <td>40.427349</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>37382</td>\n","      <td>495.586111</td>\n","      <td>25.413301</td>\n","      <td>29.318924</td>\n","      <td>26.162324</td>\n","      <td>19.260607</td>\n","      <td>12.893906</td>\n","      <td>6.580707</td>\n","      <td>7.062222</td>\n","      <td>17.334546</td>\n","      <td>32.930287</td>\n","      <td>28.302392</td>\n","      <td>28.569900</td>\n","      <td>26.804344</td>\n","      <td>30.549462</td>\n","      <td>36.595153</td>\n","      <td>42.373335</td>\n","      <td>16.398267</td>\n","      <td>22.871970</td>\n","      <td>17.174041</td>\n","      <td>15.221229</td>\n","      <td>23.433738</td>\n","      <td>14.391953</td>\n","      <td>7.383233</td>\n","      <td>8.560270</td>\n","      <td>504.413889</td>\n","      <td>26.563587</td>\n","      <td>30.255203</td>\n","      <td>24.798031</td>\n","      <td>16.237761</td>\n","      <td>11.101600</td>\n","      <td>4.788401</td>\n","      <td>5.189663</td>\n","      <td>17.842812</td>\n","      <td>30.014445</td>\n","      <td>27.767375</td>\n","      <td>30.763469</td>\n","      <td>25.199294</td>\n","      <td>29.613183</td>\n","      <td>...</td>\n","      <td>102.957039</td>\n","      <td>36.711921</td>\n","      <td>70.039055</td>\n","      <td>33.587502</td>\n","      <td>5.021387</td>\n","      <td>5.244560</td>\n","      <td>511.177236</td>\n","      <td>2.045750</td>\n","      <td>3.236005</td>\n","      <td>1.525014</td>\n","      <td>7.476288</td>\n","      <td>4.314674</td>\n","      <td>8.554956</td>\n","      <td>13.204389</td>\n","      <td>7.773852</td>\n","      <td>130.890831</td>\n","      <td>53.784638</td>\n","      <td>99.311884</td>\n","      <td>57.095034</td>\n","      <td>76.027525</td>\n","      <td>38.422912</td>\n","      <td>4.351869</td>\n","      <td>3.161614</td>\n","      <td>1000</td>\n","      <td>51.125525</td>\n","      <td>58.438255</td>\n","      <td>68.930434</td>\n","      <td>74.717029</td>\n","      <td>63.970495</td>\n","      <td>59.710034</td>\n","      <td>58.883378</td>\n","      <td>51.761414</td>\n","      <td>47.310187</td>\n","      <td>81.902582</td>\n","      <td>93.793717</td>\n","      <td>130.103014</td>\n","      <td>71.982704</td>\n","      <td>36.118530</td>\n","      <td>31.603714</td>\n","      <td>19.648989</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>19</td>\n","      <td>230108</td>\n","      <td>499.217759</td>\n","      <td>32.767222</td>\n","      <td>35.465955</td>\n","      <td>36.169972</td>\n","      <td>22.107011</td>\n","      <td>13.593617</td>\n","      <td>5.671250</td>\n","      <td>6.144941</td>\n","      <td>19.069307</td>\n","      <td>31.141899</td>\n","      <td>34.296939</td>\n","      <td>33.162689</td>\n","      <td>32.515167</td>\n","      <td>38.034314</td>\n","      <td>40.168095</td>\n","      <td>35.448572</td>\n","      <td>11.920490</td>\n","      <td>15.331931</td>\n","      <td>8.535123</td>\n","      <td>10.564604</td>\n","      <td>14.475811</td>\n","      <td>10.477689</td>\n","      <td>6.509987</td>\n","      <td>5.645175</td>\n","      <td>500.782241</td>\n","      <td>31.185356</td>\n","      <td>33.466894</td>\n","      <td>34.774975</td>\n","      <td>20.629444</td>\n","      <td>11.942218</td>\n","      <td>6.570828</td>\n","      <td>5.714708</td>\n","      <td>16.444452</td>\n","      <td>30.911572</td>\n","      <td>33.119231</td>\n","      <td>31.945869</td>\n","      <td>32.567316</td>\n","      <td>38.060389</td>\n","      <td>...</td>\n","      <td>65.673020</td>\n","      <td>57.466331</td>\n","      <td>92.510584</td>\n","      <td>27.266756</td>\n","      <td>7.497870</td>\n","      <td>4.246409</td>\n","      <td>508.847227</td>\n","      <td>4.051321</td>\n","      <td>0.747836</td>\n","      <td>1.222550</td>\n","      <td>6.054221</td>\n","      <td>3.186432</td>\n","      <td>4.714619</td>\n","      <td>3.830222</td>\n","      <td>6.678502</td>\n","      <td>167.924982</td>\n","      <td>35.284861</td>\n","      <td>67.370283</td>\n","      <td>60.691781</td>\n","      <td>106.647938</td>\n","      <td>33.073867</td>\n","      <td>4.877192</td>\n","      <td>2.490620</td>\n","      <td>1000</td>\n","      <td>37.681843</td>\n","      <td>39.309447</td>\n","      <td>43.473869</td>\n","      <td>48.558728</td>\n","      <td>44.495330</td>\n","      <td>48.188308</td>\n","      <td>46.201509</td>\n","      <td>46.504580</td>\n","      <td>47.750539</td>\n","      <td>90.247845</td>\n","      <td>122.294810</td>\n","      <td>160.998114</td>\n","      <td>100.372665</td>\n","      <td>56.045708</td>\n","      <td>39.017601</td>\n","      <td>28.859106</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>11</td>\n","      <td>928405</td>\n","      <td>493.339652</td>\n","      <td>28.576968</td>\n","      <td>30.484541</td>\n","      <td>33.556476</td>\n","      <td>21.172872</td>\n","      <td>15.826067</td>\n","      <td>7.983585</td>\n","      <td>7.764930</td>\n","      <td>19.890026</td>\n","      <td>31.149121</td>\n","      <td>29.757487</td>\n","      <td>29.253397</td>\n","      <td>34.082109</td>\n","      <td>38.370108</td>\n","      <td>40.453250</td>\n","      <td>36.642414</td>\n","      <td>12.309283</td>\n","      <td>16.434638</td>\n","      <td>9.640189</td>\n","      <td>11.470210</td>\n","      <td>14.666013</td>\n","      <td>9.220114</td>\n","      <td>7.295308</td>\n","      <td>7.340546</td>\n","      <td>506.660348</td>\n","      <td>26.972065</td>\n","      <td>28.499416</td>\n","      <td>32.284402</td>\n","      <td>20.009586</td>\n","      <td>15.343519</td>\n","      <td>7.624905</td>\n","      <td>7.173593</td>\n","      <td>18.603950</td>\n","      <td>29.942751</td>\n","      <td>30.008455</td>\n","      <td>29.901821</td>\n","      <td>35.774258</td>\n","      <td>38.770795</td>\n","      <td>...</td>\n","      <td>56.613332</td>\n","      <td>36.595069</td>\n","      <td>98.787470</td>\n","      <td>41.410251</td>\n","      <td>9.352444</td>\n","      <td>8.796601</td>\n","      <td>516.266328</td>\n","      <td>5.763291</td>\n","      <td>2.280542</td>\n","      <td>3.981419</td>\n","      <td>8.872831</td>\n","      <td>6.020566</td>\n","      <td>9.025291</td>\n","      <td>7.966014</td>\n","      <td>7.299003</td>\n","      <td>144.199786</td>\n","      <td>31.683011</td>\n","      <td>63.969508</td>\n","      <td>54.048517</td>\n","      <td>104.315718</td>\n","      <td>52.900306</td>\n","      <td>7.472109</td>\n","      <td>6.468416</td>\n","      <td>1000</td>\n","      <td>56.155414</td>\n","      <td>50.403529</td>\n","      <td>47.395876</td>\n","      <td>44.176904</td>\n","      <td>40.190822</td>\n","      <td>45.459282</td>\n","      <td>37.631856</td>\n","      <td>38.216597</td>\n","      <td>35.938423</td>\n","      <td>72.751062</td>\n","      <td>98.274145</td>\n","      <td>132.154395</td>\n","      <td>102.318122</td>\n","      <td>66.729965</td>\n","      <td>71.766845</td>\n","      <td>60.436761</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2</td>\n","      <td>15890</td>\n","      <td>498.237886</td>\n","      <td>32.473254</td>\n","      <td>34.675897</td>\n","      <td>36.752675</td>\n","      <td>26.557583</td>\n","      <td>17.747011</td>\n","      <td>4.845815</td>\n","      <td>16.362492</td>\n","      <td>20.264317</td>\n","      <td>34.675897</td>\n","      <td>33.731907</td>\n","      <td>29.452486</td>\n","      <td>31.843927</td>\n","      <td>29.452486</td>\n","      <td>30.396476</td>\n","      <td>30.522341</td>\n","      <td>2.643172</td>\n","      <td>25.361863</td>\n","      <td>8.999371</td>\n","      <td>12.523600</td>\n","      <td>16.551290</td>\n","      <td>6.104468</td>\n","      <td>7.174323</td>\n","      <td>9.125236</td>\n","      <td>501.762114</td>\n","      <td>30.144745</td>\n","      <td>34.046570</td>\n","      <td>34.990560</td>\n","      <td>22.089364</td>\n","      <td>14.977974</td>\n","      <td>9.880428</td>\n","      <td>5.349276</td>\n","      <td>21.837634</td>\n","      <td>33.731907</td>\n","      <td>32.662052</td>\n","      <td>23.599748</td>\n","      <td>38.577722</td>\n","      <td>36.186281</td>\n","      <td>...</td>\n","      <td>64.018969</td>\n","      <td>31.416716</td>\n","      <td>39.715471</td>\n","      <td>10.966212</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>515.609563</td>\n","      <td>10.274649</td>\n","      <td>43.667259</td>\n","      <td>15.016795</td>\n","      <td>13.930053</td>\n","      <td>21.932424</td>\n","      <td>5.927682</td>\n","      <td>0.592768</td>\n","      <td>1.185536</td>\n","      <td>208.753211</td>\n","      <td>0.000000</td>\n","      <td>53.645525</td>\n","      <td>34.578147</td>\n","      <td>78.838174</td>\n","      <td>21.537246</td>\n","      <td>0.000000</td>\n","      <td>5.730093</td>\n","      <td>1000</td>\n","      <td>328.835375</td>\n","      <td>143.035084</td>\n","      <td>109.404194</td>\n","      <td>91.135562</td>\n","      <td>77.434088</td>\n","      <td>88.644384</td>\n","      <td>32.177704</td>\n","      <td>30.309321</td>\n","      <td>17.023043</td>\n","      <td>15.569857</td>\n","      <td>53.975503</td>\n","      <td>12.455885</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>24327</td>\n","      <td>475.562133</td>\n","      <td>30.007810</td>\n","      <td>29.514531</td>\n","      <td>31.158795</td>\n","      <td>18.950138</td>\n","      <td>27.335882</td>\n","      <td>13.318535</td>\n","      <td>16.771488</td>\n","      <td>25.814938</td>\n","      <td>26.308217</td>\n","      <td>24.499527</td>\n","      <td>20.594401</td>\n","      <td>28.034694</td>\n","      <td>27.541415</td>\n","      <td>30.131130</td>\n","      <td>22.814157</td>\n","      <td>16.894808</td>\n","      <td>18.292432</td>\n","      <td>4.891684</td>\n","      <td>11.797591</td>\n","      <td>21.334320</td>\n","      <td>10.934353</td>\n","      <td>8.139105</td>\n","      <td>10.482180</td>\n","      <td>524.437867</td>\n","      <td>29.514531</td>\n","      <td>26.102684</td>\n","      <td>32.885272</td>\n","      <td>19.731163</td>\n","      <td>33.460764</td>\n","      <td>14.017347</td>\n","      <td>10.646607</td>\n","      <td>22.690837</td>\n","      <td>29.637851</td>\n","      <td>27.294775</td>\n","      <td>30.131130</td>\n","      <td>29.308998</td>\n","      <td>30.213343</td>\n","      <td>...</td>\n","      <td>57.262570</td>\n","      <td>25.538707</td>\n","      <td>66.906092</td>\n","      <td>19.553073</td>\n","      <td>1.795690</td>\n","      <td>5.653099</td>\n","      <td>542.631019</td>\n","      <td>2.061719</td>\n","      <td>2.261240</td>\n","      <td>6.384677</td>\n","      <td>12.702846</td>\n","      <td>20.949721</td>\n","      <td>20.218143</td>\n","      <td>17.158819</td>\n","      <td>6.650705</td>\n","      <td>191.074754</td>\n","      <td>39.638202</td>\n","      <td>75.219473</td>\n","      <td>41.965948</td>\n","      <td>66.374036</td>\n","      <td>39.106145</td>\n","      <td>0.864592</td>\n","      <td>0.000000</td>\n","      <td>1000</td>\n","      <td>143.586537</td>\n","      <td>107.012608</td>\n","      <td>64.811920</td>\n","      <td>58.976764</td>\n","      <td>48.661040</td>\n","      <td>57.517974</td>\n","      <td>46.472856</td>\n","      <td>53.871001</td>\n","      <td>26.675003</td>\n","      <td>64.707721</td>\n","      <td>87.110555</td>\n","      <td>108.679796</td>\n","      <td>64.916120</td>\n","      <td>40.637699</td>\n","      <td>16.359279</td>\n","      <td>10.003126</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2861</td>\n","      <td>19979950</td>\n","      <td>483.276184</td>\n","      <td>31.650830</td>\n","      <td>30.824702</td>\n","      <td>31.484864</td>\n","      <td>19.676576</td>\n","      <td>12.867249</td>\n","      <td>6.957375</td>\n","      <td>6.725342</td>\n","      <td>20.587289</td>\n","      <td>36.469210</td>\n","      <td>34.933871</td>\n","      <td>32.435116</td>\n","      <td>33.524108</td>\n","      <td>34.928115</td>\n","      <td>35.261650</td>\n","      <td>31.333612</td>\n","      <td>11.143872</td>\n","      <td>14.961499</td>\n","      <td>8.889862</td>\n","      <td>10.915393</td>\n","      <td>13.836321</td>\n","      <td>10.021697</td>\n","      <td>7.261179</td>\n","      <td>6.586453</td>\n","      <td>516.723816</td>\n","      <td>30.228554</td>\n","      <td>29.519894</td>\n","      <td>30.130256</td>\n","      <td>18.782730</td>\n","      <td>12.267498</td>\n","      <td>6.739957</td>\n","      <td>6.564281</td>\n","      <td>20.636188</td>\n","      <td>37.237781</td>\n","      <td>36.016507</td>\n","      <td>33.817602</td>\n","      <td>35.177465</td>\n","      <td>37.124718</td>\n","      <td>...</td>\n","      <td>56.259790</td>\n","      <td>28.012770</td>\n","      <td>104.830640</td>\n","      <td>46.524615</td>\n","      <td>16.989401</td>\n","      <td>8.370920</td>\n","      <td>528.751466</td>\n","      <td>10.664496</td>\n","      <td>6.246578</td>\n","      <td>11.740088</td>\n","      <td>11.277145</td>\n","      <td>7.400862</td>\n","      <td>8.240521</td>\n","      <td>9.871790</td>\n","      <td>11.377340</td>\n","      <td>135.112851</td>\n","      <td>21.647567</td>\n","      <td>58.133719</td>\n","      <td>38.371563</td>\n","      <td>115.299940</td>\n","      <td>63.956628</td>\n","      <td>12.952002</td>\n","      <td>6.458376</td>\n","      <td>1000</td>\n","      <td>72.572820</td>\n","      <td>46.121401</td>\n","      <td>44.665889</td>\n","      <td>43.258684</td>\n","      <td>39.156710</td>\n","      <td>39.365804</td>\n","      <td>36.342018</td>\n","      <td>36.246108</td>\n","      <td>31.909623</td>\n","      <td>64.438383</td>\n","      <td>87.488833</td>\n","      <td>117.364952</td>\n","      <td>93.519131</td>\n","      <td>63.997025</td>\n","      <td>80.800359</td>\n","      <td>102.752258</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10 rows × 190 columns</p>\n","</div>"],"text/plain":["   # Purchases  B01001001   B01001002  ...  B19001015  B19001016   B19001017\n","0           22     206252  469.226965  ...  36.440765  23.446284   21.197485\n","1            7      61399  486.538869  ...  33.000508  33.169741   24.792689\n","2            3      73170  489.859232  ...  20.125056  11.890525   16.537397\n","3           94     251724  505.585483  ...  43.731067  38.851729   40.427349\n","4            0      37382  495.586111  ...  36.118530  31.603714   19.648989\n","5           19     230108  499.217759  ...  56.045708  39.017601   28.859106\n","6           11     928405  493.339652  ...  66.729965  71.766845   60.436761\n","7            2      15890  498.237886  ...   0.000000   0.000000    0.000000\n","8            0      24327  475.562133  ...  40.637699  16.359279   10.003126\n","9         2861   19979950  483.276184  ...  63.997025  80.800359  102.752258\n","\n","[10 rows x 190 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"-62OhNRMl3_o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"98295ef9-603f-477d-92e3-96afa796bd37","executionInfo":{"status":"ok","timestamp":1574911200204,"user_tz":420,"elapsed":504,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["allvariablenames = list(alldata.columns.values)\n","allvariablenames"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['# Purchases',\n"," 'B01001001',\n"," 'B01001002',\n"," 'B01001003',\n"," 'B01001004',\n"," 'B01001005',\n"," 'B01001006',\n"," 'B01001007',\n"," 'B01001008',\n"," 'B01001009',\n"," 'B01001010',\n"," 'B01001011',\n"," 'B01001012',\n"," 'B01001013',\n"," 'B01001014',\n"," 'B01001015',\n"," 'B01001016',\n"," 'B01001017',\n"," 'B01001018',\n"," 'B01001019',\n"," 'B01001020',\n"," 'B01001021',\n"," 'B01001022',\n"," 'B01001023',\n"," 'B01001024',\n"," 'B01001025',\n"," 'B01001026',\n"," 'B01001027',\n"," 'B01001028',\n"," 'B01001029',\n"," 'B01001030',\n"," 'B01001031',\n"," 'B01001032',\n"," 'B01001033',\n"," 'B01001034',\n"," 'B01001035',\n"," 'B01001036',\n"," 'B01001037',\n"," 'B01001038',\n"," 'B01001039',\n"," 'B01001040',\n"," 'B01001041',\n"," 'B01001042',\n"," 'B01001043',\n"," 'B01001044',\n"," 'B01001045',\n"," 'B01001046',\n"," 'B01001047',\n"," 'B01001048',\n"," 'B01001049',\n"," 'B02001001',\n"," 'B02001002',\n"," 'B02001003',\n"," 'B02001004',\n"," 'B02001005',\n"," 'B02001006',\n"," 'B02001007',\n"," 'B02001008',\n"," 'B02001009',\n"," 'B02001010',\n"," 'B12001001',\n"," 'B12001002',\n"," 'B12001003',\n"," 'B12001004',\n"," 'B12001005',\n"," 'B12001006',\n"," 'B12001007',\n"," 'B12001008',\n"," 'B12001009',\n"," 'B12001010',\n"," 'B12001011',\n"," 'B12001012',\n"," 'B12001013',\n"," 'B12001014',\n"," 'B12001015',\n"," 'B12001016',\n"," 'B12001017',\n"," 'B12001018',\n"," 'B12001019',\n"," 'B13014001',\n"," 'B13014002',\n"," 'B13014003',\n"," 'B13014004',\n"," 'B13014005',\n"," 'B13014006',\n"," 'B13014007',\n"," 'B13014008',\n"," 'B13014009',\n"," 'B13014010',\n"," 'B13014011',\n"," 'B13014012',\n"," 'B13014013',\n"," 'B13014014',\n"," 'B13014015',\n"," 'B13014016',\n"," 'B13014017',\n"," 'B13014018',\n"," 'B13014019',\n"," 'B13014020',\n"," 'B13014021',\n"," 'B13014022',\n"," 'B13014023',\n"," 'B13014024',\n"," 'B13014025',\n"," 'B13014026',\n"," 'B13014027',\n"," 'B13015001',\n"," 'B13015002',\n"," 'B13015003',\n"," 'B13015004',\n"," 'B13015005',\n"," 'B13015006',\n"," 'B13015007',\n"," 'B13015008',\n"," 'B13015009',\n"," 'B13015010',\n"," 'B13015011',\n"," 'B13015012',\n"," 'B13015013',\n"," 'B13015014',\n"," 'B13015015',\n"," 'B13016001',\n"," 'B13016002',\n"," 'B13016003',\n"," 'B13016004',\n"," 'B13016005',\n"," 'B13016006',\n"," 'B13016007',\n"," 'B13016008',\n"," 'B13016009',\n"," 'B13016010',\n"," 'B13016011',\n"," 'B13016012',\n"," 'B13016013',\n"," 'B13016014',\n"," 'B13016015',\n"," 'B13016016',\n"," 'B13016017',\n"," 'B15002001',\n"," 'B15002002',\n"," 'B15002003',\n"," 'B15002004',\n"," 'B15002005',\n"," 'B15002006',\n"," 'B15002007',\n"," 'B15002008',\n"," 'B15002009',\n"," 'B15002010',\n"," 'B15002011',\n"," 'B15002012',\n"," 'B15002013',\n"," 'B15002014',\n"," 'B15002015',\n"," 'B15002016',\n"," 'B15002017',\n"," 'B15002018',\n"," 'B15002019',\n"," 'B15002020',\n"," 'B15002021',\n"," 'B15002022',\n"," 'B15002023',\n"," 'B15002024',\n"," 'B15002025',\n"," 'B15002026',\n"," 'B15002027',\n"," 'B15002028',\n"," 'B15002029',\n"," 'B15002030',\n"," 'B15002031',\n"," 'B15002032',\n"," 'B15002033',\n"," 'B15002034',\n"," 'B15002035',\n"," 'B19001001',\n"," 'B19001002',\n"," 'B19001003',\n"," 'B19001004',\n"," 'B19001005',\n"," 'B19001006',\n"," 'B19001007',\n"," 'B19001008',\n"," 'B19001009',\n"," 'B19001010',\n"," 'B19001011',\n"," 'B19001012',\n"," 'B19001013',\n"," 'B19001014',\n"," 'B19001015',\n"," 'B19001016',\n"," 'B19001017']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Pl4DziW7BQPF","colab_type":"code","colab":{}},"source":["listofallpredictors = allvariablenames[8:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"plqLwDxVBdP8","colab_type":"code","colab":{}},"source":["#load predictors into dataframe\n","predictors = alldata[listofallpredictors]  \n","\n","#load target into dataframe\n","target = alldata['# Purchases']   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1PhuVowBlpS","colab_type":"code","colab":{}},"source":["# split data into train and test sets, with 30% retained for test\n","\n","pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtLslqFZB0Bz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"544581db-4fcb-4ea8-e8e0-6ab386c8cb38","executionInfo":{"status":"ok","timestamp":1574911380245,"user_tz":420,"elapsed":714,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["# Fitting\n","model = LassoLarsCV(precompute=False, cv=10)\n","model.fit(pred_train, tar_train)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.201e-01, previous alpha=7.200e-01, with an active set of 12 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.546e-02, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.539e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 116 iterations, alpha=3.652e-02, previous alpha=3.599e-02, with an active set of 99 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.435e-02, previous alpha=6.382e-02, with an active set of 61 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.644e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.822e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.062e-01, previous alpha=1.984e-01, with an active set of 34 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n","  ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n","            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n","            positive=False, precompute=False, verbose=False)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"xWYEYp9DCIfj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"outputId":"9bbf9195-0a74-4e65-e35f-910b1b6e357f","executionInfo":{"status":"ok","timestamp":1574911471111,"user_tz":420,"elapsed":337,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["predictors_model=pd.DataFrame(listofallpredictors)\n","predictors_model.columns = ['label']\n","predictors_model['coeff'] = model.coef_\n","\n","for index, row in predictors_model.iterrows():\n","    if row['coeff'] > 0:\n","        print(row.values)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["['B01001014' 0.8558761066941788]\n","['B01001036' 2.5053482381631653]\n","['B01001037' 0.8892493223320962]\n","['B01001038' 1.5316387928880384]\n","['B02001005' 0.41252295298457853]\n","['B13014026' 0.48004105312075906]\n","['B13014027' 0.6978957445987839]\n","['B13016001' 875149895.329212]\n","['B19001017' 1.4834348068681533]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TsWkworMCexj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yCMsCBWPCmKu","colab_type":"text"},"source":["Question 1: The above lines of code are going through each variable and assigning a value to the coeffcicent and pulling variables where the coefficient is greater than 0 to show the significant predictors."]},{"cell_type":"code","metadata":{"id":"9HRfz4ALDM-h","colab_type":"code","colab":{}},"source":["# Question 2:\n","# ['B01001014' 0.8557908775529921] Males aged 40 to 44 Years.\n","# If there is one more Males aged 40 to 44 Years, we will sell 0. 8557908775529921 unit more Bobo Bars.\n","\n","# ['B01001036' 2.505392496591849] Females aged 30 to 34 Years.\n","# If there is one more Females aged 30 to 34 Years, we will sell 2.505392496591849 unit more Bobo Bars.\n","        \n","# ['B01001037' 0.8894214357013622] Females aged 35 to 39 Years.\n","# If there is one more Females aged 35 to 39 Years, we will sell 0.8894214357013622 unit more Bobo Bars.\n","\n","# ['B01001038' 1.5315839680821497] Females aged 40 to 44 Years.\n","# If there is one more Females aged 40 to 44 Years, we will sell 1.5315839680821497 unit more Bobo Bars.\n","        \n","# ['B02001005' 0.4125408937426837] Asian Alone\n","# If there is one more Asian Alone, we will sell 0.4125408937426837 unit more Bobo Bars.\n","        \n","# ['B13014026' 0.4800240326923769] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree\n","# If there is one more Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree, we will sell 0.4800240326923769 unit more Bobo Bars.\n","\n","# ['B13014027' 0.6977454940063235] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree\n","# If there is one more Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree, we will sell 0.6977454940063235 unit more Bobo Bars.\n","\n","# ['B13016001' 874922971.7249781] Women 15 to 50 Years Who Had a Birth in the Past 12 Months\n","# If there is one more Women 15 to 50 Years Who Had a Birth in the Past 12 Months, we will sell 874922971.7249781 unit more Bobo Bars.\n","\n","# ['B19001017' 1.4834465563617387] Household with income $200,000 or More.\n","# If there is one more Household with income $200,000 or More, we will sell 1.4834465563617387 unit more Bobo Bars.\n","\n","# Question 3:\n","# If I had to report only two census variables to my boss that most steeply predicted sales, the first one would be \n","# Women 15 to 50 Years Who Had a Birth in the Past 12 Months and the second one would be Females aged 30 to 34 Years."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YAAAEwnoDoLb","colab_type":"text"},"source":["Question 2:\n","['B01001014' 0.8557908775529921] Males aged 40 to 44 Years.\n","For every increase of 1 male aged 40 to 44 Years, we will sell 0. 8557908775529921 unit more Bobo Bars.\n","\n","['B01001036' 2.505392496591849] Females aged 30 to 34 Years.\n","For every increase of 1 female aged 30 to 34 Years, we will sell 2.505392496591849 unit more Bobo Bars.\n","        \n","['B01001037' 0.8894214357013622] Females aged 35 to 39 Years.\n","For every increase of 1 female aged 35 to 39 Years, we will sell 0.8894214357013622 unit more Bobo Bars.\n","\n","['B01001038' 1.5315839680821497] Females aged 40 to 44 Years.\n","For every increase of 1 female aged 40 to 44 Years, we will sell 1.5315839680821497 unit more Bobo Bars.\n","        \n","['B02001005' 0.4125408937426837] Asian Alone\n","For every increase of 1 Asian alone, we will sell 0.4125408937426837 unit more Bobo Bars.\n","        \n","['B13014026' 0.4800240326923769] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree\n","For every increase of 1 Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree, we will sell 0.4800240326923769 unit more Bobo Bars.\n","\n","['B13014027' 0.6977454940063235] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree\n","For every increase of 1 Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree, we will sell 0.6977454940063235 unit more Bobo Bars.\n","\n","['B13016001' 874922971.7249781] Women 15 to 50 Years Who Had a Birth in the Past 12 Months\n","For every increase of 1 Women 15 to 50 Years Who Had a Birth in the Past 12 Months, we will sell 874922971.7249781 unit more Bobo Bars.\n","\n","['B19001017' 1.4834465563617387] Household with income $200,000 or More.\n","For every increase of 1 Household with income 200,000 or More, we will sell 1.4834465563617387 unit more Bobo Bars.\n"]},{"cell_type":"markdown","metadata":{"id":"vLolgpYZEpKA","colab_type":"text"},"source":["Question 3: The two variables that most steeply predict sales are Women 15 to 50 Years Who Had a Birth in the Past 12 Months and the second one would be Females aged 30 to 34 Years."]},{"cell_type":"code","metadata":{"id":"nxY1_urNFJq1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"41a44f7c-01ad-4177-8d16-c328c997dd5a","executionInfo":{"status":"ok","timestamp":1574912173837,"user_tz":420,"elapsed":365,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["from sklearn.metrics import mean_squared_error\n","train_error = mean_squared_error(tar_train, model.predict(pred_train))\n","print ('training data MSE')\n","print(train_error)\n","\n","test_error = mean_squared_error(tar_test, model.predict(pred_test))\n","print ('testing data MSE')\n","print(test_error)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["training data MSE\n","22025.491066757\n","testing data MSE\n","41549.54803776253\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iu6i7h04FaRX","colab_type":"text"},"source":["Question 4: The training and test set mean squared errors are not similar. This means that the smaller the MSE the better the fit. The MSE for training data has a lesser value therefore is a better fit than the test set."]},{"cell_type":"code","metadata":{"id":"INRTJ5NyFGI9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"8e90227d-ad1b-407f-b34b-f0df1d680412","executionInfo":{"status":"ok","timestamp":1574912474391,"user_tz":420,"elapsed":855,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["#r squared\n","rsquared_train=model.score(pred_train,tar_train)\n","print ('training data R-square')\n","print(rsquared_train)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["training data R-square\n","0.2400221219784492\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KOia5LgfGUXY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"e1b87e68-82bd-4c10-9026-8f8086cc0a24","executionInfo":{"status":"ok","timestamp":1574912482195,"user_tz":420,"elapsed":598,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["rsquared_test = model.score(pred_test, tar_test)\n","print ('testing data R-square')\n","print(rsquared_test)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["testing data R-square\n","0.1758628512005107\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XYyneDkNF-HT","colab_type":"text"},"source":["Question 5: The R-Squared value for both sets are rather low so I would tell my boss that census data does not do a good job of predicting sales. "]},{"cell_type":"code","metadata":{"id":"4VghYRe2GyBr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"a672b50f-b12f-4e77-832b-1183ce4ed54e","executionInfo":{"status":"ok","timestamp":1574912601268,"user_tz":420,"elapsed":331,"user":{"displayName":"Dylan Bernstein","photoUrl":"","userId":"17772000167247866529"}}},"source":["print(\"y interecept:\")\n","print(model.intercept_)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["y interecept:\n","22.19738813257551\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qHEtCC6YG3ky","colab_type":"text"},"source":["Question 6: According to the intercept, the baseline sales number is 22.194697684317433. This means when value of every\n","variable is 0, 22.194697684317433 Bobo bars will be sold."]}]}